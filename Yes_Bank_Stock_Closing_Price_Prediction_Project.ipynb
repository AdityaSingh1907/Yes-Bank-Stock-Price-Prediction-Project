{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaSingh1907/Yes-Bank-Stock-Price-Prediction-Project/blob/main/Yes_Bank_Stock_Closing_Price_Prediction_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Yes Bank Stock Closing Price Prediction**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - **Regression**\n",
        "##### **Contribution** - **Individual**\n",
        "##### **Name** - **Aditya Singh**\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The objective of this project is to develop a machine learning model that can accurately predict the closing price of Yes Bank's stock. Predicting stock prices is a challenging task due to the complex and dynamic nature of financial markets. However, by leveraging historical stock data and utilizing advanced machine learning techniques, we aim to build a robust predictive model.\n",
        "\n"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes Bank is a well-known bank in the Indian financial domain. Since 2018, it has been in the news because of the fraud case involving Rana Kapoor. Owing to this fact, it was interesting to see how that impacted the stock prices of the company and whether Time series models or any other predictive models can do justice to such situations. This dataset has monthly stock prices of the bank since its inception and includes closing, starting, highest, and lowest stock prices of every month. The main objective is to predict the stockâ€™s closing price of the month.**"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -"
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "\n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "\n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "\n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "\n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from scipy.stats import *\n",
        "import plotly.express as px\n",
        "import math\n",
        "from scipy.stats import mannwhitneyu\n",
        "from datetime import datetime                           # to convert to date\n",
        "\n",
        "from dateutil.relativedelta import relativedelta        # working with dates with style\n",
        "from datetime import datetime                           # computational cost\n",
        "from scipy.optimize import minimize                     # for function minimization\n",
        "import copy                                             # create copies\n",
        "\n",
        "from sklearn.preprocessing import (MinMaxScaler,        # scale the data\n",
        "StandardScaler)\n",
        "from sklearn.feature_selection import VarianceThreshold #remove constant and quasi-constant features.\n",
        "from sklearn.model_selection import train_test_split    # split train and test data\n",
        "from sklearn.model_selection import (cross_val_score,   # split train and test data on a timeseries\n",
        "TimeSeriesSplit)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression       # regression model\n",
        "from xgboost import XGBRegressor                        # xgboost model\n",
        "from sklearn.ensemble import RandomForestRegressor      # random forest model\n",
        "from sklearn.svm import SVR                             # support vector regressor\n",
        "from sklearn.linear_model import (Lasso, Ridge,         # regularization\n",
        "ElasticNet, LassoCV, RidgeCV, ElasticNetCV)\n",
        "from sklearn.model_selection import GridSearchCV        # grid search to optimize parameters\n",
        "\n",
        "from sklearn.metrics import (r2_score,                  # import required metrics\n",
        "mean_squared_error,  mean_absolute_percentage_error,\n",
        "mean_absolute_error)\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller          # statistics and econometrics\n",
        "import statsmodels.formula.api as smf\n",
        "import statsmodels.tsa.api as smt\n",
        "import statsmodels.api as sm\n",
        "import scipy.stats as scs\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mYpPSC9-WY-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "file_path = '/content/drive/MyDrive/data_YesBank_StockPrices.csv'\n",
        "df = pd.read_csv(file_path)"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "len(df[df.duplicated()])"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "sns.heatmap(df.isnull(), cbar=True)"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have a total of 185 entries.\n",
        "No null values.\n",
        "Date column is of 'object' datatype we have to convert it to 'datetime'."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert string object to datetime object\n",
        "df['Date'] = pd.to_datetime(df['Date'].apply(lambda x: datetime.strptime(x, \"%b-%y\")))"
      ],
      "metadata": {
        "id": "pIiTcHZkOlL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "3txOjSF4PAl-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe(include='all')\n"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Date**\t- Date of record\n",
        "\n",
        "**Open**\t- Opening price\n",
        "\n",
        "**High**  -\tHighest price in the day\n",
        "\n",
        "**Low** - Lowest price in the day\n",
        "\n",
        "**Close** - Occupations of the speaker"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "\n",
        "for i in df.columns.tolist():\n",
        "  print(\"No. of unique values in \",i,\"is\",df[i].nunique(),\".\")"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "# Copy of data\n",
        "df=df.copy()"
      ],
      "metadata": {
        "id": "tVH93mAXMB23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna()  # Drop rows with NaN value\n",
        "df.dropna(axis=1)  # Drop columns with any NaN value"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fill Missing Values with Mean/Median/Mode\n",
        "\n",
        "mean_value = df['Date'].mean()\n",
        "df['Date'].fillna(mean_value, inplace=True)\n",
        "\n",
        "mean_value = df['Open'].mean()\n",
        "df['Open'].fillna(mean_value, inplace=True)\n",
        "\n",
        "mean_value = df['High'].mean()\n",
        "df['High'].fillna(mean_value, inplace=True)\n",
        "\n",
        "mean_value = df['Low'].mean()\n",
        "df['Low'].fillna(mean_value, inplace=True)\n",
        "\n",
        "mean_value = df['Close'].mean()\n",
        "df['Close'].fillna(mean_value, inplace=True)\n",
        "\n",
        "print(df.head())\n",
        "df.shape"
      ],
      "metadata": {
        "id": "HgqOfzPHtN44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BIFURCATE DEPENDENT AND INDEPENDENT VARIABLES\n",
        "\n",
        "indep_var=df[['High','Low','Open']]\n",
        "dep_var=df['Close']"
      ],
      "metadata": {
        "id": "wSZpMmM6wRyd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, I made a copy of our dataset then droped the missing values and filled all missing values with Mean and Separated all the varibles to as a dependent and independent variables."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1 - Pie Chart on Independent variables (UNIVARIATE)"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#plots for independent variables\n",
        "for var in indep_var:\n",
        "    plt.figure(figsize=(15,6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    fig = sns.distplot(df[var].dropna())\n",
        "    fig.set_ylabel(' ')\n",
        "    fig.set_xlabel(var)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    fig = sns.boxplot(y=df[var])\n",
        "    fig.set_title('')\n",
        "    fig.set_ylabel(var)"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two types of charts are used for visualizing the independent variables: a distribution plot and a box plot.\n",
        "\n",
        "Distribution plot:-This plot helps in understanding the data distribution and identifying any outliers or unusual patterns.\n",
        "\n",
        "Box plot :-This plot helps in identifying potential outliers, skewness, and variability in the data."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have ganerated some insights with help of independent variables and we can clearly see that our data is right skewed."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights shows the possitive skewed distribution of the independent variables like 'Opening price', 'Low', 'High' columns that will help me to understand and methodes to be applied to tackle the skewness of the data."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2 - Dependent Variable"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Dependent variable 'Closing price'\n",
        "plt.figure(figsize=(15,10))\n",
        "sns.distplot(df['Close'],color=\"y\")\n",
        "plt.title('Close Data Distribution')\n",
        "plt.xlabel('Closing Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two types of charts are used for visualizing the independent variables: a distribution plot and a box plot.\n",
        "\n",
        "Distribution plot:-This plot helps in understanding the data distribution and identifying any outliers or unusual patterns.\n",
        "\n",
        "Box plot :-This plot helps in identifying potential outliers, skewness, and variability in the data."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have ganerated some insights with help of dependent variables and we can clearly see that our data is right skewed."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights shows the possitive skewed distribution of the dependent variables like 'Closing price' columns that will help me to understand and methodes to be applied to tackle the skewness of the data."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3- Visualize The Data"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization\n",
        "# visualise the data\n",
        "fig = px.line(df, df['Date'], df['Close'], title='Monthly closing price')\n",
        "fig.update_layout(\n",
        "    xaxis=dict(title='Year'),\n",
        "    yaxis=dict(title='Closing price'),\n",
        "    autosize=False,\n",
        "    width=1400,\n",
        "    height=400)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " A line plot is well-suited for this purpose as it provides a clear representation of how the closing prices have evolved over time, allowing you to spot trends and changes."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that from 2006 to 2018, the stock prices more or less, kept increasing but there was a sudden dip fall after that. This can be attributed to the Yes bank fraud case against Rana Kapoor"
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4 - Check for skewness in the dataset"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "\n",
        "numeric_features = df.describe().columns\n",
        "for col in numeric_features[0:4]:\n",
        "    fig = plt.figure(figsize=(9, 6))\n",
        "    ax = fig.gca()\n",
        "    feature = df[col]\n",
        "    feature.hist(bins=50, ax = ax)\n",
        "    ax.axvline(feature.mean(), color='magenta', linestyle='dashed', linewidth=2)\n",
        "    ax.axvline(feature.median(), color='cyan', linestyle='dashed', linewidth=2)\n",
        "    ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used histogram to visualizing the distribution of data in the numeric columns, along with lines indicating the mean and median values. This can help you understand the central tendency and spread of each feature's data."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above chart we can clearly see that all numerical variables are possitively skewed. So i have to transform this columns for handling the skewness of data."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the gained insights shows the possitive skewed distribution of all the numbric columns that will help me to understand and methodes to be applied to tackle the skewness of the data."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5 - Relationship between dependent & independent variables"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "# scatter plot to see the relationship between dependent & independent variables\n",
        "for col in df.describe().columns[:-1]:\n",
        "  fig = plt.figure(figsize=(20,5))\n",
        "  ax = fig.gca()\n",
        "  plt.scatter(df[col], df['Close'])\n",
        "  plt.xlabel(col)\n",
        "  plt.ylabel('Close')\n",
        "  ax.set_title('{} vs Close'.format(col))\n",
        "  z = np.polyfit(df[col], df['Close'], 1)\n",
        "  y_hat = np.poly1d(z)(df[col])\n",
        "  plt.plot(df[col], y_hat, \"r--\", lw=1)\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scatter plots with the regression lines provide visual insight into how each independent variable relates to the dependent variable 'Close'. The slope and direction of the regression lines can indicate the strength and direction of the relationship"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that the independent variables(Open,High,Low) And dependent variable 'Close' are highly correlated therefore we can say that the closing price is very much dependent on independent variables(Open,High,Low)."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The high correlation between the independent variables ('Open', 'High', 'Low') and the dependent variable ('Close') indicates a strong positive relationship. This information can be valuable for building predictive models, understanding stock price dynamics, and making informed decisions in the financial domain."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.title('Correlation Heatmap')\n",
        "cor = sns.heatmap(df.corr(), cmap='coolwarm', annot=True )"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A correlation matrix is a table showing correlation coefficients between variables. Each cell in the table shows the correlation between two variables. A correlation matrix is used to summarize data, as an input into a more advanced analysis, and as a diagnostic for advanced analyses. The range of correlation is [-1,1].\n",
        "\n",
        "Thus to know the correlation between all the variables along with the correlation coeficients, i used correlation heatmap."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every feature is extremely corelated with each other, so taking just one feature or average of these features would suffice for our regression model as linear regression assumes there is no multi colinearity in the features."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.pairplot(df)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pair plot is used to understand the best set of features to explain a relationship between two variables or to form the most separated clusters. It also helps to form some simple classification models by drawing some simple lines or make linear separation in our data-set.\n",
        "\n",
        "Thus, I used pair plot to analyse the patterns of data and realationship between the features. It's exactly same as the correlation map but here you will get the graphical representation."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every feature is extremely corelated with each other, so taking just one feature or average of these features would suffice for our regression model as linear regression assumes there is no multi colinearity in the features."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1: The closing prices of Yes Bank stock in the last quarter of the year have a different distribution compared to the closing prices in the first quarter.   \n",
        "2: There is a significant difference in the mean closing prices of Yes Bank stock between two consecutive years.  \n",
        "3: The closing prices of Yes Bank stock on Fridays have a different distribution compared to the closing prices on other weekdays."
      ],
      "metadata": {
        "id": "b3TJcaDU_Zrm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "close_data = df['Close'].values\n",
        "high_data = df['High'].values\n",
        "low_data = df['Low'].values\n",
        "open_data = df['Open'].values\n"
      ],
      "metadata": {
        "id": "mGhvpd6YJ_JE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1 -The closing prices of Yes Bank stock in the last quarter of the year have a different distribution compared to the closing prices in the first quarter."
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the closing price distributions between the last quarter and the first quarter.\n",
        "\n",
        "Alternative Hypothesis (Ha): There is a significant difference in the closing price distributions between the last quarter and the first quarter."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hypothesis Testing - Statement 1: Closing prices in the last quarter vs. first quarter\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "t_stat, p_value = stats.mannwhitneyu(close_data[:2], close_data[-2:])\n",
        "print(\"Hypothesis Statement 1:\")\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n",
        "if p_value < 0.05:\n",
        "    print(\"Conclusion: Reject the null hypothesis. The closing prices in the last quarter and first quarter have different distributions.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in the closing price distributions between the last quarter and first quarter.\")\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value obtained from the Mann-Whitney U test indicates the probability of observing the data if the null hypothesis (no difference in distributions) is true. If the p-value is below a chosen significance level (commonly 0.05), you reject the null hypothesis and conclude that there is a significant difference between the closing price distributions in the last quarter and the first quarter."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for required columns to know the data distibution\n",
        "\n",
        "columns_to_visualize = ['Close', 'High', 'Low', 'Open']\n",
        "plt.figure(figsize=(12, 8))\n",
        "for i, col in enumerate(columns_to_visualize):\n",
        "    plt.subplot(2, 2, i+1)\n",
        "    plt.hist(df[col], bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.title(f'{col} Distribution')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "PyK24FRVJVUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the Mann-Whitney U test is a suitable non-parametric test for comparing two independent groups when the data is not normally distributed."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2 -There is a significant difference in the mean closing prices of Yes Bank stock between two consecutive years."
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the closing price distributions between the last quarter and the first quarter.\n",
        "\n",
        "Alternative Hypothesis (Ha): There is a significant difference in the closing price distributions between the last quarter and the first quarter."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sGJTOmmpdMrt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "#Hypothesis Testing - Statement 2: Closing prices for two consecutive years\n",
        "t_stat, p_value = stats.kruskal(close_data[:2], close_data[-2:])\n",
        "print(\"\\nHypothesis Statement 2:\")\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n",
        "if p_value < 0.05:\n",
        "    print(\"Conclusion: Reject the null hypothesis. There is a significant difference in the mean closing prices between two consecutive years.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in the mean closing prices between two consecutive years.\")\n"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The P-Value Obtain by the Kruskal-Wallis test and\n",
        "\n",
        "Conclusion: Fail to reject the null hypothesis. There is no significant difference in the mean closing prices between two consecutive years.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Kruskal-Wallis test is often used when the data is not normally distributed, or when the sample sizes are small, and the assumptions of the one-way ANOVA are not met."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3 -The closing prices of Yes Bank stock on Fridays have a different distribution compared to the closing prices on other weekdays."
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Null Hypothesis (H0): There is no significant difference in the closing price distributions between the last quarter and the first quarter.\n",
        "\n",
        "Alternative Hypothesis (Ha): There is a significant difference in the closing price distributions between the last quarter and the first quarter."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Hypothesis Testing - Statement 3: Closing prices on Fridays vs. other weekdays\n",
        "t_stat, p_value = stats.mannwhitneyu(close_data[::5], np.concatenate([close_data[1::5], close_data[2::5], close_data[3::5], close_data[4::5]]))\n",
        "print(\"\\nHypothesis Statement 3:\")\n",
        "print(\"t-statistic:\", t_stat)\n",
        "print(\"p-value:\", p_value)\n",
        "if p_value < 0.05:\n",
        "    print(\"Conclusion: Reject the null hypothesis. The closing prices on Fridays have different distributions compared to other weekdays.\")\n",
        "else:\n",
        "    print(\"Conclusion: Fail to reject the null hypothesis. There is no significant difference in the closing price distributions between Fridays and other weekdays.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The p-value obtained from the Mann-Whitney U test and  \n",
        "\n",
        "Conclusion: Fail to reject the null hypothesis. There is no significant difference in the closing price distributions between Fridays and other weekdays.\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "the Mann-Whitney U test is a suitable non-parametric test for comparing two independent groups when the data is not normally distributed."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a copy of the dataset for further feature engineering\n",
        "df=df.copy()"
      ],
      "metadata": {
        "id": "T4txtZrhhvxk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Missing Values/Null Values Count\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualizing the missing values\n",
        "# Checking Null Value by plotting Heatmap\n",
        "sns.heatmap(df.isnull(), cbar=False)"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are no missing values to handle in the given dataset."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Extract the 'Close', 'High', 'Low', and 'Open' columns from the DataFrame\n",
        "close_data = df['Close'].values\n",
        "high_data = df['High'].values\n",
        "low_data = df['Low'].values\n",
        "open_data = df['Open'].values\n",
        "\n",
        "# Function to remove outliers using Z-score method\n",
        "def remove_outliers_zscore(data):\n",
        "    z_scores = np.abs(stats.zscore(data))\n",
        "    threshold = 3\n",
        "    data_no_outliers = data[z_scores <= threshold]\n",
        "    return data_no_outliers\n",
        "\n",
        "# Remove outliers for each column\n",
        "close_data_no_outliers = remove_outliers_zscore(close_data)\n",
        "high_data_no_outliers = remove_outliers_zscore(high_data)\n",
        "low_data_no_outliers = remove_outliers_zscore(low_data)\n",
        "open_data_no_outliers = remove_outliers_zscore(open_data)\n",
        "\n",
        "# Print the lengths of the original and outlier-removed data for comparison\n",
        "print(\"Original Data Lengths:\")\n",
        "print(\"Close Data:\", len(close_data))\n",
        "print(\"High Data:\", len(high_data))\n",
        "print(\"Low Data:\", len(low_data))\n",
        "print(\"Open Data:\", len(open_data))\n",
        "\n",
        "print(\"\\nOutlier-Removed Data Lengths:\")\n",
        "print(\"Close Data:\", len(close_data_no_outliers))\n",
        "print(\"High Data:\", len(high_data_no_outliers))\n",
        "print(\"Low Data:\", len(low_data_no_outliers))\n",
        "print(\"Open Data:\", len(open_data_no_outliers))\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Z-score method to handlling our outliers, this technique is based on standardizing data points by calculating how many standard deviations they are from the mean. Data points with Z-scores greater than a threshold (usually 3 or -3) are considered outliers.\n",
        "We used this method because it is straightforward to implement and provides a statistical approach to identify extreme values."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "categorical_columns=list(set(df.columns.to_list()).difference(set(df.describe().columns.to_list())))\n",
        "print(\"Categorical Columns are :-\", categorical_columns)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# One-Hot Encoding\n",
        "one_hot_encoded = pd.get_dummies(df, columns=['Date'])\n",
        "\n",
        "# Label Encoding (for ordinal categorical variables)\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['Date'] = label_encoder.fit_transform(df['Date'])\n",
        "\n",
        "# Print the encoded data\n",
        "print(\"One-Hot Encoded Data:\")\n",
        "print(one_hot_encoded.head())\n",
        "\n",
        "print(\"\\nLabel Encoded Data:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "_TZkf44QSgXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used One-hot encoding technique , this technique is used to convert categorical variables into binary vectors. Each category in the original variable is represented by a binary column, where a value of 1 indicates the presence of the category, and 0 indicates its absence.\n",
        "One-hot encoding is used when dealing with nominal categorical variables (categories without any inherent order) or when the categorical variable has a small number of unique categories.\n",
        "\n",
        "We used one-hot encoding because it is a simple and effective way to represent categorical variables with multiple categories, allowing machine learning algorithms to interpret the data correctly."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)\n",
        "\n",
        "**There are no text columns in the given dataset which I am working on. So, Skipping this part.**"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Creating a new feature based on average of other features in the dataset\n",
        "df['OHL'] = df[['Open', 'High', 'Low']].mean(axis=1).round(2)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Linear regression also assumes a linear relationship between the target variables and independent variables, let's check if such relationship exists through a scatter plot"
      ],
      "metadata": {
        "id": "L9gH6by2ggAA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scatter plot to see the relationship between dependent & independent variables\n",
        "fig = plt.figure(figsize=(20,5))\n",
        "ax = fig.gca()\n",
        "plt.scatter(df['OHL'], df['Close'])\n",
        "plt.xlabel('OHL')\n",
        "plt.ylabel('Close')\n",
        "ax.set_title('OHL vs Close')\n",
        "z = np.polyfit(df['OHL'], df['Close'], 1)\n",
        "y_hat = np.poly1d(z)(df['OHL'])\n",
        "plt.plot(df['OHL'], y_hat, \"r--\", lw=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Or_QRbj44tbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the shape of dataset\n",
        "df.shape"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the target variable (y) from the features (X)\n",
        "X = df.drop(columns=['Close'])  # Replace 'Closing_Price' with the column name of the target variable\n",
        "y = df['Close']\n",
        "\n",
        "# Split the data into training and testing sets (if needed)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the VarianceThreshold with threshold (default is 0)\n",
        "# For quasi-constant features, you can set a threshold close to 0 to remove features with very low variance\n",
        "variance_threshold = VarianceThreshold(threshold=0.01)  # Adjust the threshold value as per your data\n",
        "\n",
        "# Fit the VarianceThreshold on the training data\n",
        "variance_threshold.fit(X_train)\n",
        "\n",
        "# Get the indices of non-constant and non-quasi-constant features\n",
        "non_constant_features = variance_threshold.get_support(indices=True)\n",
        "\n",
        "# Filter the original data to keep only non-constant and non-quasi-constant features\n",
        "X_train_filtered = X_train.iloc[:, non_constant_features]\n",
        "X_test_filtered = X_test.iloc[:, non_constant_features]\n",
        "\n",
        "# Print the original and filtered shape to see the difference\n",
        "print(\"Original data shape:\", X_train.shape)\n",
        "print(\"Filtered data shape:\", X_train_filtered.shape)\n"
      ],
      "metadata": {
        "id": "2TbvVoEoeKoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the VarianceThreshold class from scikit-learn to remove constant and quasi-constant features. The threshold value is set to 0.01, which means features with a variance below 0.01 will be considered quasi-constant and removed. we can adjust the threshold value based on our data and problem requirements.\n",
        "\n",
        "We have replaced 'Close' column with the column name of your target variable.\n",
        "\n",
        "After dropping constant and quasi-constant features, we will be left with a filtered DataFrame containing only the informative features, which can improve the performance of our machine learning models."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate the target variable (y) from the features (X)\n",
        "X = df.drop(columns=['Close'])  # Replace 'Closing_Price' with the column name of the target variable\n",
        "y = df['Close']\n",
        "\n",
        "# Assuming you have already performed feature selection and stored the selected features in a variable 'selected_features'\n",
        "X_selected = X[X_train.columns]\n",
        "\n",
        "# Split the data into training and testing sets (if needed)\n",
        "# Replace 'test_size' and 'random_state' with appropriate values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_selected, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Regression model\n",
        "rf_model = RandomForestRegressor()\n",
        "\n",
        "# Fit the model on the training data\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Get the feature importance scores\n",
        "feature_importance = rf_model.feature_importances_\n",
        "\n",
        "# Print the feature importance scores\n",
        "print(\"Feature Importance Scores:\")\n",
        "for feature, importance in zip(X_train.columns, feature_importance):\n",
        "    print(f\"{feature}: {importance}\")"
      ],
      "metadata": {
        "id": "SyrmKjoCi7IF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I found 5 important featuares by using the the Random Forest Regression model and the feature importance scores suggest that the 'Low' price, followed by the 'High' price, have the most significant impact on predicting the Yes Bank stock's closing price. This aligns with the general understanding that low and high prices are strong indicators of market sentiment and can heavily influence the closing price. However, it's essential to interpret these results carefully and consider additional factors to build a robust predictive model for closing price prediction."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " #Separate the target variable (Close) from the features (X)\n",
        "X = df.drop(columns=['Close'])\n",
        "y = df['Close']\n",
        "\n",
        "# Calculate mean and median difference for each column\n",
        "mean_median_diff = abs(X.mean() - X.median())\n",
        "\n",
        "# Define a threshold for determining symmetric features\n",
        "threshold = 0.1\n",
        "\n",
        "# Separate symmetric and non-symmetric features\n",
        "symmetric_feature = list(mean_median_diff[mean_median_diff < threshold].index)\n",
        "non_symmetric_feature = list(mean_median_diff[mean_median_diff >= threshold].index)\n",
        "\n",
        "# Print symmetric and non-symmetric features\n",
        "print(\"Symmetric Distributed Features: \", symmetric_feature)\n",
        "print(\"Skew Symmetric Distributed Features: \", non_symmetric_feature)"
      ],
      "metadata": {
        "id": "Kp-W5FiyyLwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "#Transform the required columns using the exponential transformation\n",
        "columns_to_transform = ['Close', 'Open', 'High', 'Low']\n",
        "\n",
        "for col in columns_to_transform:\n",
        "    df[col] = df[col] ** 0.25\n",
        "\n",
        "# Print the updated DataFrame to check the changes\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing code of hist plot for each columns to know the data distibution\n",
        "for col in df.loc[:,non_symmetric_feature]:\n",
        "  fig=plt.figure(figsize=(9,6))\n",
        "  ax=fig.gca()\n",
        "  feature= (df[col])\n",
        "  sns.distplot(df[col])\n",
        "  ax.axvline(feature.mean(),color='magenta', linestyle='dashed', linewidth=2)\n",
        "  ax.axvline(feature.median(),color='cyan', linestyle='dashed', linewidth=2)\n",
        "  ax.set_title(col)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O1GfLYcB22SR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the features, I got to know that there are 4 features which aren't symmetric so aren't following gaussian distribution and rest are having symmetric curve. Thus, for those two columns I have used Exponential transformation to achieve gaussian distribution.\n",
        "\n",
        "I tried with other transformations and found exponetial tranformation with no infinity value and working fine. So, I am continuing with Exponential transformation with a power of 0.25."
      ],
      "metadata": {
        "id": "yKGduOFQ3XHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Select only the numerical columns for scaling\n",
        "numerical_columns = ['Open', 'High', 'Low', 'OHL']\n",
        "X_numerical = df[numerical_columns]\n",
        "\n",
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Scale the numerical features\n",
        "X_scaled = scaler.fit_transform(X_numerical)\n",
        "\n",
        "# Now, X_scaled contains the scaled numerical features"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used Min-Max Scaling and Standardization (Z-score scaling) for scale the data. The code demonstrates how to perform both scaling methods on the feature columns of the dataset:\n",
        "\n",
        "Min-Max Scaling:\n",
        "\n",
        "Method: Min-Max Scaling scales the data to a specific range, usually [0, 1].\n",
        "Reason for Using: Min-Max Scaling is employed to bring all the features to a common scale within the range of [0, 1]. It is suitable when the features have different ranges, and we want to preserve the relationships between the data points while maintaining interpretability.\n",
        "\n",
        "Standardization (Z-score Scaling):\n",
        "\n",
        "Method: Standardization scales the data to have a mean of 0 and a standard deviation of 1.\n",
        "Reason for Using: Standardization is applied to transform the data to have a mean of 0 and a standard deviation of 1. It is useful when the features have different units and scales, and we want to give them equal importance.\n"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As per my knowledge, for this dataset dimensionality reduction is not required.\n",
        "\n",
        "Dimensionality reduction is a technique used to reduce the number of features or variables in a dataset while preserving most of its important information. In your case, you have a relatively small dataset with 185 rows and 5 columns. Since the number of features (5 columns) is already small compared to the number of samples (185 rows), the need for dimensionality reduction may not be as critical as it would be for larger datasets."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# 5 fold time-series cross-validation\n",
        "# split into 80:20 ratio\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# function for splitting time-series dataset\n",
        "def timeseries_train_test_split(X, y, test_size):\n",
        "    \"\"\"\n",
        "        Perform train-test split with respect to time series structure\n",
        "    \"\"\"\n",
        "\n",
        "    # get the index after which test set starts\n",
        "    test_index = int(len(X)*(1-test_size))\n",
        "    scaler = StandardScaler()\n",
        "    X_train = X.iloc[:test_index]\n",
        "    y_train = y.iloc[:test_index]\n",
        "    X_test = X.iloc[test_index:]\n",
        "    y_test = y.iloc[test_index:]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# choose appropriate dependent and independent variables\n",
        "y = df.dropna().Close\n",
        "X = df.dropna().drop(['Date','Close','Open','High','Low'], axis=1)\n",
        "\n",
        "# split the dataset into train and test sets\n",
        "X_train, X_test, y_train, y_test = timeseries_train_test_split(X, y, test_size=0.2)\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "xaq3cOBL3-AT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The choice of an 80-20 split is common in many machine learning tasks and is often a good starting point for dividing data into training and testing sets. The training set (80%) is used to train the machine learning model, and the testing set (20%) is used to evaluate the model's performance and generalize its performance on unseen data."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 - Implementing Linear Regression Model"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lr = LinearRegression()\n",
        "# Fit the Algorithm\n",
        "lr.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "7H4245NsLU_n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.coef_"
      ],
      "metadata": {
        "id": "EM1wRSExAsdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr.intercept_\n"
      ],
      "metadata": {
        "id": "RSVGC4w7L2IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make predictions\n",
        "lr_y_pred = lr.predict(X_test)\n",
        "\n",
        "# evaluate predictions\n",
        "lr_mae = round(mean_absolute_error(y_test, lr_y_pred),2)\n",
        "print('mean absolute error: {}\\n'.format(lr_mae))\n",
        "lr_mse = round(mean_squared_error(y_test, lr_y_pred),2)\n",
        "print('mean squared error: {}\\n'.format(lr_mse))\n",
        "lr_rmse = round(np.sqrt(lr_mse),2)\n",
        "print('root mean squared error: {}\\n'.format(lr_rmse))\n",
        "lr_r2 = round(r2_score(y_test, lr_y_pred),2)\n",
        "print('r2_score: {}\\n'.format(lr_r2))\n",
        "lr_mape = round(mean_absolute_percentage_error(lr_y_pred, y_test),2)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(lr_mape))\n"
      ],
      "metadata": {
        "id": "T_nWMTx0_QCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_report = pd.DataFrame(data={'model':['linear regression'], 'mae':[lr_mae], 'mse':[lr_mse],'rmse':[lr_rmse],'r2_score':[lr_r2],'mape':[lr_mape]})\n",
        "model_report"
      ],
      "metadata": {
        "id": "bPB52b4zSkb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Visulization chart for Linear Regression Model\n",
        "metrics = ['MAE', 'MSE', 'RMSE', 'R2 Score', 'MAPE']\n",
        "scores = [lr_mae, lr_mse, lr_rmse, lr_r2, lr_mape]\n",
        "\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel('Evaluation Metric')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Metric Scores for Linear Regression Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "z7ZZPJEtOzhn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for homoscadacity\n",
        "plt.scatter(lr_y_pred, y_test)\n",
        "plt.title('Variance of residuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xKYp-2g-OjRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# function to plot model performance\n",
        "def plotModelResults(model, X_train=X_train, X_test=X_test, plot_intervals=False, plot_anomalies=False):\n",
        "    \"\"\"\n",
        "        Plots modelled vs fact values, prediction intervals and anomalies\n",
        "    \"\"\"\n",
        "\n",
        "    prediction = model.predict(X_test)\n",
        "\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.plot(prediction, \"g\", label=\"prediction\", linewidth=2.0)\n",
        "    plt.plot(y_test.values, label=\"actual\", linewidth=2.0)\n",
        "\n",
        "    if plot_intervals:\n",
        "        cv = cross_val_score(model, X_train, y_train,\n",
        "                                    cv=tscv,\n",
        "                                    scoring=\"neg_mean_absolute_error\")\n",
        "        mae = cv.mean() * (-1)\n",
        "        deviation = cv.std()\n",
        "\n",
        "        scale = 1.96\n",
        "        lower = prediction - (mae + scale * deviation)\n",
        "        upper = prediction + (mae + scale * deviation)\n",
        "\n",
        "        plt.plot(lower, \"r--\", label=\"upper bond / lower bond\", alpha=0.5)\n",
        "        plt.plot(upper, \"r--\", alpha=0.5)\n",
        "\n",
        "        if plot_anomalies:\n",
        "            anomalies = np.array([np.NaN]*len(y_test))\n",
        "            anomalies[y_test>upper] = y_test[y_test>upper]\n",
        "            plt.plot(anomalies, \"o\", markersize=10, label = \"Anomalies\")\n",
        "\n",
        "    error = mean_absolute_percentage_error(prediction, y_test)\n",
        "    plt.title(\"Mean absolute percentage error {0:.2f}\".format(error))\n",
        "    plt.legend(loc=\"best\")\n",
        "    plt.tight_layout()\n",
        "    plt.grid(True);\n",
        "\n",
        "# function to plot coefficients\n",
        "def plotCoefficients(model):\n",
        "    \"\"\"\n",
        "        Plots sorted coefficient values of the model\n",
        "    \"\"\"\n",
        "\n",
        "    coefs = pd.DataFrame(model.coef_, X_train.columns)\n",
        "    coefs.columns = [\"coef\"]\n",
        "    coefs[\"abs\"] = coefs.coef.apply(np.abs)\n",
        "    coefs = coefs.sort_values(by=\"abs\", ascending=False).drop([\"abs\"], axis=1)\n",
        "\n",
        "    plt.figure(figsize=(12, 5))\n",
        "    coefs.coef.plot(kind='bar')\n",
        "    plt.grid(True, axis='y')\n",
        "    plt.hlines(y=0, xmin=0, xmax=len(coefs), linestyles='dashed')\n",
        "\n",
        "plotModelResults(lr, plot_intervals=True)"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have implemented a Linear Regression model for predicting the closing stock price of Yes Bank.\n",
        "\n",
        "We You have used several evaluation metrics to assess the performance of our Linear Regression model on the testing data.\n",
        "\n",
        "The R-squared value of 0.87 indicates that approximately 87% of the variance in the closing stock price can be explained by the features used in the model.\n",
        "\n",
        "The MAE of 0.26 suggests that, on average, your model's predictions are off by around 0.26 units of the closing stock price.\n",
        "\n",
        "The MSE of 0.1 indicates the average squared difference between predicted and actual values is 0.1.\n",
        "\n",
        "The RMSE of 0.32 provides a measure of error in the same units as the original data.\n",
        "\n",
        "The MAPE of 0.08 (8%) indicates an average percentage difference of 8% between predicted and actual values.\n",
        "\n",
        "Overall, Our Linear Regression model seems to provide reasonably good performance on the test data. The R-squared value indicates that the model captures a significant portion of the variance, and the other metrics provide insights into the magnitude and distribution of errors."
      ],
      "metadata": {
        "id": "IP5uBakMyP0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Implementing Lasso regression\n"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "lasso  = Lasso(alpha=0.0001 , max_iter= 3000)\n",
        "# Fit the Algorithm\n",
        "lasso.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "ouDpiRxXskLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.score(X_train, y_train)"
      ],
      "metadata": {
        "id": "VX7l5xJ6tdV6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lasso.coef_"
      ],
      "metadata": {
        "id": "tL1xHnattj0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and fit lasso regression\n",
        "lasso = LassoCV(cv=tscv)\n",
        "lasso.fit(X_train, y_train)\n",
        "\n",
        "plotModelResults(lasso,\n",
        "                 X_train,\n",
        "                 X_test,\n",
        "                 plot_intervals=True, plot_anomalies=True)\n"
      ],
      "metadata": {
        "id": "0stesyqH3abd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# make predictions\n",
        "l_y_pred = lasso.predict(X_test)\n",
        "\n",
        "# evaluate predictions\n",
        "l_mae = round(mean_absolute_error(y_test, l_y_pred),2)\n",
        "print('mean absolute error: {}\\n'.format(l_mae))\n",
        "l_mse = round(mean_squared_error(y_test, l_y_pred),2)\n",
        "print('mean squared error: {}\\n'.format(l_mse))\n",
        "l_rmse = round(np.sqrt(l_mse),2)\n",
        "print('root mean squared error: {}\\n'.format(l_rmse))\n",
        "l_r2 = round(r2_score(y_test, l_y_pred),2)\n",
        "print('r2_score: {}\\n'.format(l_r2))\n",
        "l_mape = round(mean_absolute_percentage_error(l_y_pred, y_test),2)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(l_mape))\n",
        "\n",
        "# Check for homoscadacity\n",
        "plt.scatter(l_y_pred, y_test)\n",
        "plt.title('Variance of residuals')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_report = pd.DataFrame(data={'model': ['lasso'],\n",
        "                                  'mae': [l_mae],\n",
        "                                  'mse': [l_mse],\n",
        "                                  'rmse': [l_rmse],\n",
        "                                  'r2_score': [l_r2],\n",
        "                                  'mape': [l_mape]})\n",
        "\n",
        "# Display the model report\n",
        "print(model_report)"
      ],
      "metadata": {
        "id": "NnsuOrBEgeQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Visulization chart for Lasso Regression Model\n",
        "metrics = ['MAE', 'MSE', 'RMSE', 'R2 Score', 'MAPE']\n",
        "scores = [lr_mae, lr_mse, lr_rmse, lr_r2, lr_mape]\n",
        "\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel('Evaluation Metric')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Metric Scores for Lasso Regression Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "n9u6ZVokkv7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is Lasso Regression. Lasso Regression is a type of linear regression that performs both variable selection and regularization to prevent overfitting. It works by adding a penalty term to the linear regression cost function, encouraging the model to use fewer features by shrinking some feature coefficients to zero.\n",
        "\n",
        "the MAE is 0.25, which indicates that, on average, the model's predictions are off by 0.25 units from the actual values.\n",
        "\n",
        "The MSE is 0.09, indicating that the model's predictions have a squared average difference of 0.09.\n",
        "\n",
        "The RMSE of 0.3 suggests that, on average, the model's predictions are off by 0.3 units from the actual values.\n",
        "\n",
        "The R2 score measures the proportion of the variance in the dependent variable (closing prices) that's predictable from the independent variables (features). An R2 score of 0.88 means that approximately 88% of the variability in the closing prices can be explained by the model's features.\n",
        "\n",
        "The MAPE of 0.07 implies that, on average, the model's predictions are off by 7% from the actual values.\n",
        "\n",
        "Overall, these evaluation metrics suggest that the model is performing well. The low values of MAE, MSE, and RMSE indicate accurate predictions. The high R2 score indicates that the model's features are explaining a significant portion of the variance in the closing prices. The low MAPE implies that the percentage difference between predicted and actual values is relatively small.\n"
      ],
      "metadata": {
        "id": "JAS9qQbrmiml"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "optimal_lasso= Lasso()\n",
        "parameters = {'alpha': [1e-15,1e-13,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1e-1,1,5,10,20,30,40,45,50,55,60,100,0.0014]}\n",
        "lasso_regressor = GridSearchCV(lasso, parameters, scoring='neg_mean_squared_error', cv=5)\n",
        "\n",
        "# Fit the Algorithm\n",
        "lasso_regressor.fit(X_train, y_train)\n"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The best fit alpha value is found out to be :\" ,lasso_regressor.best_params_)\n",
        "print(\"\\nUsing \",lasso_regressor.best_params_, \" the negative mean squared error is: \", lasso_regressor.best_score_)"
      ],
      "metadata": {
        "id": "ogrfubr0ztWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_lasso = lasso_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "g6llfMHA3-cW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(y_pred_lasso)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dtnvu9og4Bis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate predictions\n",
        "l_mae_grid = round(mean_absolute_error(y_test, y_pred_lasso),2)\n",
        "print('mean absolute error: {}\\n'.format(l_mae_grid))\n",
        "l_mse_grid = round(mean_squared_error(y_test, y_pred_lasso),2)\n",
        "print('mean squared error: {}\\n'.format(l_mse_grid))\n",
        "l_rmse_grid = round(np.sqrt(l_mse_grid),2)\n",
        "print('root mean squared error: {}\\n'.format(l_rmse_grid ))\n",
        "l_r2_grid = round(r2_score(y_test, y_pred_lasso),2)\n",
        "print('r2_score: {}\\n'.format(l_r2_grid))\n",
        "l_mape_grid = round(mean_absolute_percentage_error(y_pred_lasso, y_test),2)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(l_mape_grid))\n",
        "\n",
        "# Check for homoscadacity\n",
        "plt.scatter(y_pred_lasso, y_test)\n",
        "plt.title('Variance of residuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mw2-fxNl3906"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_report = pd.DataFrame(data={'model': ['optimal_lasso'],\n",
        "                                  'mae': [l_mae_grid],\n",
        "                                  'mse': [l_mse_grid],\n",
        "                                  'rmse': [l_rmse_grid],\n",
        "                                  'r2_score': [l_r2_grid],\n",
        "                                  'mape': [l_mape_grid]})\n",
        "\n",
        "# Display the model report\n",
        "print(model_report)"
      ],
      "metadata": {
        "id": "agovRbs5BMdY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV with the Lasso regression model and a range of alpha values. The reason for using GridSearchCV in this case is that it systematically explores different values of the regularization parameter alpha to find the one that minimizes the negative mean squared error, which is the scoring metric we chose.\n",
        "\n",
        "GridSearchCV is a good choice when we have a relatively small parameter space to search through, as it guarantees that we'll find the best combination of hyperparameters from the given grid. However, it might be computationally expensive if the parameter space is large.\n",
        "\n",
        "Overall, GridSearchCV is a well-established and robust method for hyperparameter tuning, providing a systematic approach to optimizing model performance."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 -Implementing Ridge Regression"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "ridge = Ridge()\n",
        "# Fit the Algorithm\n",
        "ridge.fit(X_train,y_train)\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "r_y_pred = ridge.predict(X_test)\n",
        "print(r_y_pred)"
      ],
      "metadata": {
        "id": "sq_BI-EH-c7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize and fit ridge regression\n",
        "ridge = RidgeCV(cv=tscv)\n",
        "ridge.fit(X_train, y_train)\n",
        "\n",
        "plotModelResults(ridge,\n",
        "                 X_train,\n",
        "                 X_test,\n",
        "                 plot_intervals=True, plot_anomalies=True)\n"
      ],
      "metadata": {
        "id": "ISUvwiKn7fkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# evaluate predictions\n",
        "r_mae = round(mean_absolute_error(r_y_pred, y_test),2)\n",
        "print('mean absolute error: {}\\n'.format(r_mae))\n",
        "r_mse = round(mean_squared_error(r_y_pred, y_test),2)\n",
        "print('mean squared error: {}\\n'.format(r_mse))\n",
        "r_rmse = round(np.sqrt(r_mse),2)\n",
        "print('root mean squared error: {}\\n'.format(r_rmse))\n",
        "r_r2 = round(r2_score(r_y_pred, y_test),2)\n",
        "print('r2_score: {}\\n'.format(r_r2))\n",
        "r_mape = round(mean_absolute_percentage_error(r_y_pred, y_test),2)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(r_mape))"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#evaluation Metric Score chart\n",
        "ridge_model_report = pd.DataFrame(data={'model': ['ridge'],\n",
        "                                  'mae': [r_mae],\n",
        "                                  'mse': [r_mse],\n",
        "                                  'rmse': [r_rmse],\n",
        "                                  'r2_score': [r_r2],\n",
        "                                  'mape': [r_mape]})\n",
        "\n",
        "# Display the Ridge regression model report\n",
        "print(ridge_model_report)"
      ],
      "metadata": {
        "id": "rFUIXDtZ_cnX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Visulization chart for Ridge Regression Model\n",
        "metrics = ['MAE', 'MSE', 'RMSE', 'R2 Score', 'MAPE']\n",
        "scores = [lr_mae, lr_mse, lr_rmse, lr_r2, lr_mape]\n",
        "\n",
        "plt.bar(metrics, scores)\n",
        "plt.xlabel('Evaluation Metric')\n",
        "plt.ylabel('Score')\n",
        "plt.title('Evaluation Metric Scores for Ridge Regression Model')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qzaqKmUOCH0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ML model used in this case is Ridge Regression.Ridge Regression is suitable when you believe that most of the independent variables are relevant and potentially contributing to the target variable, even though they might be correlated.\n",
        "\n",
        "The MAE of 0.26 indicates that, on average, the model's predictions deviate from the actual values by approximately 0.26 units. This suggests that the model's predictions are relatively close to the true values.\n",
        "\n",
        "The MSE value of 0.1 means that, on average, the squared differences between the predicted and actual values are around 0.1. Lower MSE values are better, indicating that the model's predictions have relatively smaller errors.\n",
        "\n",
        "The RMSE value of 0.32 is the square root of the MSE. It provides an estimate of the average magnitude of prediction errors. The lower RMSE value suggests that the model's predictions have relatively smaller and consistent errors.\n",
        "\n",
        "The R2 score of 0.89 indicates that the model explains about 89% of the variance in the target variable. In other words, around 89% of the variability in the actual data is captured by the model's predictions. This is a relatively high R2 score, implying that the model is performing well in explaining the variability in the data.\n",
        "\n",
        "The MAPE value of 0.08 means that, on average, the model's predictions deviate from the actual values by about 8% in terms of percentage. This suggests that the model's predictions are generally within a reasonable percentage of the actual values.\n",
        "\n",
        "Overall, the evaluation metrics collectively indicate that the model is performing well. It is making predictions that are close to the actual values with relatively small errors. The R2 score also indicates a good fit to the data, and the MAPE suggests that the percentage errors are moderate. This model seems to be a good candidate for predicting the stock's closing price."
      ],
      "metadata": {
        "id": "8hR3nGbtE7-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "optimal_ridge = Ridge()\n",
        "parameters = {'alpha': [1e-15,1e-10,1e-8,1e-5,1e-4,1e-3,1e-2,1,5,10,20,30,40,45,50,55,60,100]}\n",
        "ridge_regressor = GridSearchCV(ridge, parameters, scoring='neg_mean_squared_error', cv=3)\n",
        "# Fit the Algorithm\n",
        "ridge_regressor.fit(X_train,y_train)\n",
        "\n",
        "print(\"The best fit alpha value is found out to be :\" ,ridge_regressor.best_params_)\n",
        "print(\"\\nUsing \",ridge_regressor.best_params_, \" the negative mean squared error is: \", ridge_regressor.best_score_)"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Predict on the model\n",
        "y_pred_ridge = ridge_regressor.predict(X_test)"
      ],
      "metadata": {
        "id": "0W_ca9S7Ki_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_ridge\n"
      ],
      "metadata": {
        "id": "18YHBnp7MB-N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(y_pred_ridge)\n",
        "plt.plot(np.array(y_test))\n",
        "plt.legend([\"Predicted\",\"Actual\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "C4tjygs0K3g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate predictions\n",
        "r_mae_grid = round(mean_absolute_error(y_pred_ridge, y_test),2)\n",
        "print('mean absolute error: {}\\n'.format(r_mae_grid))\n",
        "r_mse_grid = round(mean_squared_error(y_pred_ridge, y_test),2)\n",
        "print('mean squared error: {}\\n'.format(r_mse_grid))\n",
        "r_rmse_grid = round(np.sqrt(r_mse_grid),2)\n",
        "print('root mean squared error: {}\\n'.format(r_rmse_grid))\n",
        "r_r2_grid = round(r2_score(y_pred_ridge, y_test),2)\n",
        "print('r2_score: {}\\n'.format(r_r2_grid))\n",
        "r_mape_grid = round(mean_absolute_percentage_error(y_pred_ridge, y_test),2)\n",
        "print('mean absolute percentage error: {}\\n\\n\\n'.format(r_mape_grid))"
      ],
      "metadata": {
        "id": "n7fLn51a7Li_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "model_report = pd.DataFrame(data={'model': ['optimal_ridge'],\n",
        "                                  'mae': [r_mae_grid],\n",
        "                                  'mse': [r_mse_grid],\n",
        "                                  'rmse': [r_rmse_grid],\n",
        "                                  'r2_score': [r_r2_grid],\n",
        "                                  'mape': [r_mape_grid]})\n",
        "\n",
        "# Display the model report\n",
        "print(model_report)"
      ],
      "metadata": {
        "id": "9YwXCETuCiLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV with the Lasso regression model and a range of alpha values. The reason for using GridSearchCV in this case is that it systematically explores different values of the regularization parameter alpha to find the one that minimizes the negative mean squared error, which is the scoring metric we chose.\n",
        "\n",
        "GridSearchCV is a good choice when we have a relatively small parameter space to search through, as it guarantees that we'll find the best combination of hyperparameters from the given grid. However, it might be computationally expensive if the parameter space is large.\n",
        "\n",
        "Overall, GridSearchCV is a well-established and robust method for hyperparameter tuning, providing a systematic approach to optimizing model performance."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create dataframe with evaluation metrics\n",
        "# Define the evaluation metrics\n",
        "metrics_data = {'model': ['Linear Regression', 'Ridge Regression', 'Optimal Ridge Regression', 'Lasso Regression', 'Optimal Lasso Regression'],\n",
        "                'mae': [lr_mae, r_mae, r_mae_grid, l_mae, l_mae_grid],\n",
        "                'mse': [lr_mse, r_mse, r_mse_grid, l_mse, l_mse_grid],\n",
        "                'rmse': [lr_rmse, r_rmse, r_rmse_grid, l_rmse, l_rmse_grid],\n",
        "                'r2_score': [lr_r2, r_r2, r_r2_grid, l_r2, l_r2_grid],\n",
        "                'mape': [lr_mape, r_mape, r_mape_grid, l_mape, l_mape_grid]}\n",
        "\n",
        "# Create a dataframe\n",
        "model_report = pd.DataFrame(data=metrics_data)\n",
        "\n",
        "# Display the model report\n",
        "model_report\n"
      ],
      "metadata": {
        "id": "mIcnYpTAQB1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the dataframe by 'r2_score' in decreasing order\n",
        "model_report_sorted = model_report.sort_values(by='r2_score', ascending=False)\n",
        "\n",
        "# Display the sorted model report\n",
        "model_report_sorted"
      ],
      "metadata": {
        "id": "C6kg2UnOGBEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I'm considering the MAE, MAPE And R2 Score Evaluation metrics for a positive business impact .\n",
        "\n",
        "For instance, if small prediction errors have a direct financial impact, MAE and MAPE could be important. If understanding how well our model explains the variability in the target variable is crucial, R2 would be relevant. Ultimately, a combination of these metrics, along with domain knowledge, can provide a comprehensive view of our model's performance and its potential positive impact on our business."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Ridge Regression model, as the final prediction model.\n",
        "\n",
        "Here's why:\n",
        "\n",
        "Ridge Regression has slightly better performance in terms of R-squared (R2) score and negative mean squared error. It also performs well in terms of other metrics such as MAE, MSE, RMSE, and MAPE. The choice of the best alpha value, 100, after hyperparameter tuning further indicates that the model is able to mitigate multicollinearity effectively.\n",
        "\n",
        "Therefore, based on the given information, Ridge Regression with the best-fit alpha value of 100 could be considered as the final prediction model. This model shows promising performance in terms of both predictive accuracy and generalization to unseen data. It's important to note that this decision could be influenced by the specific requirements of your problem, the business context, and the balance between model complexity and interpretability.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The data set dose not have any null values/missing values as well as dupolicate values which made the analysis easy and smooth.\n",
        "*   I started with univariate analysis in which it can be seen that all the variables were possitively skewed.\n",
        "* identified the features ('Open', 'High', 'Low', etc.) and the target variable ('Close').\n",
        "* I conducted feature selection, filtering out features with low variance.\n",
        "*  Applied transformations like exponential transformation on specific columns to possibly normalize the data.\n",
        "*   Performed data scaling using Min-Max Scaling and Standardization to bring features to a common scale.\n",
        "*  Implemented Linear Regression, Lasso Regression (with hyperparameter tuning), and Ridge Regression.\n",
        "*   You evaluated these models using various performance metrics such as MAE, MSE, RMSE, R2 score, and MAPE.\n",
        "*   Linear Regression showed an R2 score of 0.87, indicating it explains about 87% of the variance in the target variable.\n",
        "*   Lasso Regression with hyperparameter tuning improved the performance slightly with an R2 score of 0.88.\n",
        "*   Ridge Regression also exhibited similar performance with an R2 score of 0.89.\n",
        "*   I selected the Ridge Regression model, as a final prediction model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}